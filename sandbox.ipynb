{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7dc57d7-07bc-4f45-881f-8bd11bee4f56",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e698586-881b-4e17-8a0c-3f95a7c096ea",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641b0290-a43a-4dd3-a0ff-d637591769f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from data_ingestion.ingest import *\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "from retrieval_eval.query_gen.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d63c5f-02ce-475a-a5b8-8c03c159f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79cd45-d995-4785-9a5e-653ad69b82d5",
   "metadata": {},
   "source": [
    "### ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5422d81c-57a2-4c01-8a53-1bb3fe56d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('data/videos.db')\n",
    "\n",
    "# # View as DataFrame\n",
    "# df = pd.read_sql_query(\"SELECT * FROM videos\", conn)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7371fa3-370b-4388-980d-f115a8dacf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e64ec40-e2fb-4df6-a038-68f75eaab5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_id = '982V2ituTdc'\n",
    "# irow = (df['video_id'] == video_id).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f393b515-e82f-4e06-9531-5970dab65e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['transcript'][irow])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88816fa7-433b-4aaf-a23b-ea3b25b8e8b0",
   "metadata": {},
   "source": [
    "### query gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce0826b-3008-479b-941f-f5e38a35e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ingestion.database import get_video_by_id\n",
    "# from query_gen.functions import get_all_comments, generate_queries\n",
    "\n",
    "# # Get video data\n",
    "# video_data = get_video_by_id(video_id)\n",
    "# print(f\"Title: {video_data['title']}\")\n",
    "\n",
    "# # Get comments\n",
    "# comments = get_all_comments(video_id)\n",
    "# print(f\"Comments: {len(comments)}\")\n",
    "\n",
    "# # Generate queries\n",
    "# result = generate_queries(\n",
    "#   video_title=video_data[\"title\"],\n",
    "#   transcript=video_data[\"transcript\"],\n",
    "#   comments=comments\n",
    "# )\n",
    "\n",
    "# # Display results\n",
    "# for i, q in enumerate(result.queries, 1):\n",
    "#   print(f\"\\n{i}. [{q.query_type.value} / {q.difficulty.value}]\")\n",
    "#   print(f\"   {q.query}\")\n",
    "#   print(f\"   â†’ {q.grounding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32154a97-9de8-4fed-b85a-dee44e751d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c14983-6fbe-46dd-aeea-6b2c734fe2fa",
   "metadata": {},
   "source": [
    "### answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf18479c-1e51-4dca-9faa-f2df86bd2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.answer import generate_answer\n",
    "\n",
    "# response = generate_answer(\"why is overfitting a problem for decision trees\")\n",
    "# print(response.answer)\n",
    "# for citation in response.citations:\n",
    "#     print(f\"- {citation.title} (https://youtu.be/{citation.video_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8705f9-8263-4e3c-a6ab-1c3356ee7a29",
   "metadata": {},
   "source": [
    "### user message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ced3ba8a-3343-4079-ab2c-9c67ff5582f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Question\n",
      "\n",
      "does picking sym4 change how well i find peaks\n",
      "\n",
      "## Retrieved Videos\n",
      "\n",
      "### Dimensionality Reduction & Segmentation with Decision Trees | Python Code (ID: 4vvoIA0MalQ)\n",
      "\n",
      "hey everyone welcome back I'm Shaw and in this video I'm going to continue the series on decision trees and talk about a couple of applications so in the last two videos of the series we talked about how we can train predictive models using decision trees so in the first video we just talked about models employing a single decision tree then in the second video we expanded this idea to tree ensembles so if you haven't already be sure to check those out because we're going to build upon those ideas in this video the whole point of the discussion today is that we can use machine learning models specifically decision trees for more than just making predictions so this is what I'll call Next Level uses of decision trees not because they're anything profound or groundbreaking but because they go beyond this obvious task of just using a machine learning model to make a prediction and I think for those who are just getting started in data science it may be easy to think that all there is to data science is getting some data training a model and making predictions and that somehow through this process there's going to be immediate real world impact in value and the reality is it's not so straightforward so what I really like about data science is the critical thinking and the creativity that's required to use these tools and techniques to solve real world problems and provide value and so that's all I mean by Next Level so toward that end in this video I'll talk about two ways we can use decision trees for more than just making predictions so the first is reducing predictor count and the second is called predictor segmentation starting with the first one reduce predictor count so this goes back to the previous video of the series where we talked about tree ensembles where we stitch together a bunch of decision trees which made our machine learning model more robust while decision tree ensembles give us so many great things like I was talking about in the previous video all these great things come at a cost which which is that tree ensembles are a bit of a black box so we know what we put into the tree Ensemble and we can see what comes out of it but what happens in between is a bit of a mystery and this is a problem for a lot of machine learning models we may get this great predictive performance but a lot of times it's not so easy to interpret what's happening under the hood however we don't have to use our tree Ensemble to make our final predictions rather what we can do is take the feature importance ranking from our tree Ensemble model and use that to inform a simpler set of predictor variables and so this is all motivated by Occam's razor which is a very popular idea and in this context what it implies is that simpler models are better so let's say that we have our true Ensemble model and it has 30 predictor variables to it but following the logic here what we want to do is take these 30 variables and just keep the handful of variables that are most important so not only does this help us in interpreting what the model is doing but in a lot of cases it can actually lead to an improvement in predictive performance so walking through what this might look like we take our tree Ensemble and we spit out our feature importance ranking then what we can do is take the top predictor train a machine learning model from it so this could be a decision tree could be a logistic regression model linear model neural network it really doesn't matter what kind of model we use it for we use the one predictor to develop a model and then we assess that model's performance then we can do the same exact thing for the top two predictors and now we have a model with two variables we grab its performance metrics for three models four models and so on and so forth so we just keep doing this until we have all the predictors in our original tree Ensemble model once we go through this process we can plot a chart that looks something like this we're on the x-axis we have the number of variables included in the model and then on the y-axis we have some performance measures so here I put AUC just as an example each of these points corresponds to a different model and we can see the gain and predictive performance here so for example we kind of see that we have pretty big gains until we hit about three variables so what this tells us is maybe we don't need all six variables and we can just get away with using three of them without a major loss in predictive performance and so the upside of that like I mentioned earlier is that a model with three input variables is a little easier to interpret than a model with 6 or 60 input variables so now I'm going to walk through some example code of doing this so here I'm going to use the same data set that I used in the previous video this series so there's a bit of overlap between the code here and from the previous video so I'm not going to spend too much time on any recurring details but if you want to learn more check out that video and also the code is available at the GitHub linked down here and Linked In the description below First Step as always is importing modules this is a lot of the same stuff as the example code from the previous video with the only addition of this import here where we're bringing in a logistic regression model from sklearn and then as always we're going to load in our data next we're doing some data prep this is very similar to what we saw in the previous video this is something new where all I'm doing here is since Y is a Boolean variable meaning it can only take values of zero or one all I'm doing is switching the meaning of zero and one so originally zero meant the tumor was malignant and one meant the tumor was benign and all I'm doing with this transformation is switching one means the tumor is malignant and zero means the tumor is benign and the reason I'm doing this is that later down the line when it comes to interpreting what the coefficients of our logistic regression model mean it's just a bit more intuitive to talk about things in terms of risk of breast cancer as opposed to the opposite which would be like safety from breast cancer and then this should also be a review where we're using smoke to balance our imbalance data set so we have way more benign cases than malignant cases so all smote is doing is synthetically over sampling the minority class and then we're using this train test split function to create our training and testing data sets Okay so now we can train our random Forest so this is one of the tree Ensemble methods we saw in the previous video so we can fit that model with just a couple lines of code so next we have something new everything up until this point we basically did in the previous video but now we're kind of going into some novelty so all we're doing here is pulling the feature importances from our random forest model and then we're sorting them in descending order so what this looks like is this where these are all the names of our features and these numbers here quantify their relative importance and so now what we can do is exactly the process I was describing before or retrain a model using the top predictor and assess its performance and we train another model using the top two predictors assess the performance top three sets performance so on and so forth so what this looks like in code could be something like this where we're initializing all these lists to store our classifiers and then to store our different performance measures we can ignore this I equals zero here this is just left over from an earlier version of the code what we're doing here is for I corresponding to the number of elements in this series we're going to go through one by one and do the following block of code so what we're doing here is listing the feature names up until I plus one we train our logistic regression model using this line of code and then we're just appending things to these lists from before so we're pending the classifier to the classifier list we're appending the AUC value for the training data set and then we're appending the AUC value for the testing data set if this is confusing and complicated don't worry what matters is this final result which is just like what we saw before which is our number of variables plotted on the x-axis and then the performance of the models on the y-axis so this this red dashed line is the AUC value for the random forest model we trained originally so this is a tree Ensemble model that uses all 30 predictor variables but what's really remarkable is that once we hit five variables the logistic regression model actually outperforms this more sophisticated model that uses six times as many variables and then you can see after five variables the logistic regression models just keep getting better and better but let's say for our purposes we really value being able to interpret what the model is doing as well as the model's accuracy so let's say once we beat our random force model we're satisfied so that's the model we're going to use and then since logistic regression is a linear model we can easily interpret the relationship between our predictor variables and the target variable by looking at the model coefficients and so the bars here are just showing the coefficient values looking at worst perimeter which is about 0.3 the way to interpret this is a unit increase and worst perimeter translates to a 0.3 increase in the log odds that the tumor is malignant so I know that was a mouthful making that a bit more qualitative as the worst perimeter increases the probability that the tumor is malignant also increases so now we kind of have the concrete quantification of the interaction between our predictor variables and the probability that the tumor is malignant and so there's a small technical detail here that I don't want to spend too much time on but I talk about more in the blog and it has to do with the resampling since we use mode to synthetically oversample the minority class we can't immediately translate our logistic regression model outputs to probabilities and that's just because the y-intercept for our logistic regression model is biased due to the over sampling and so there's a simple fix there we can just adjust our y-intercept to make it not biased and then everything works perfectly and so if you want to learn more about that check out the blog okay next we have predictor variable segmentation and this actually goes back to the first blog in this series where we used a decision tree model for sepsis survival prediction and there the final decision tree we had looked like this and what's interesting here is even though we had three predictor variables the vast majority of these splits are only using age so here the initial split is splitting on ages less than or equal to 58.5 years and then 44.5 78.5 56.5 67 86 so this is really interesting what this is indicating is that when it comes to sepsis survival age is the most important risk factor we have in our data set and so the other ones we had were the sex of the patient and also the number of previous sepsis episodes and so sometimes in cases like this where there's one predictor variable that has this outsized impact on our Target it can make sense to do segmentation on that predictor variable and what that means is all we're doing is taking this continuous variable age and we're going to partition it into discrete sections so kind of looking at this visually let's say we have ages on our data set ranging from zero to a hundred all segmentation does is split these ages into some number of subcategories so let's say we want to split it into five subcategories and then the result looks like this and so what you can do now is instead of training a decision Tree on all of your data you can train separate decision trees for each age group and what this can translate to is better model performance especially if there are systematic differences between these age groups which requires separate model development so the question is how do we come up with these segments so we can definitely do it manually so we just kind of look at the data and say okay let's do this age group and that age group or use some kind of subject matter expertise but another way we can do it is using a decision tree so this picture here is showing how we can come up with these segments using a decision tree you notice that age is actually being split into these different sections based on the sepsis outcome of dead or alive but maybe we wouldn't want to use this decision tree directly because it has this other variable involved in the splits so what we can do is train another decision tree model but now instead of using the three predictors of age sex and number of sepsis episodes we can just use the one variable we care about which is age so now I'm going to walk through what that looks like I'm going to use the same data set from the first video of this series and then as always we're going to start by importing our modules so these shouldn't be anything new next we're going to load in our data just like we did in the first video and then we're going to do some some data prep so here all we're doing is keeping the variables of age and the sepsis survival flag and now we're going to do a little bit of data prep so what we're doing here is we're grouping the data based on age so you can imagine that we have all these different patients and there can be multiple patients of the same age so all we're doing here is reshuffling the data to have only unique age values but then for each of these unique age values we're going to have a percent of patients that are alive and we're going to name this column percent alive and then on the flip side we can take 1 minus the percent alive and create a new column called percent not alive and so the result of that is a data frame that looks like this so now we only have unique age values starting from zero going all the way up to a hundred and then for each age value we have the percentage of them that are live and the percentage of them that are dead next all we're doing here is grabbing the variable names and creating separate data frames for our input and Target variable so here the predicted variable is age the target variable is going to be percent not alive and as a first pass to the relationship we can just plot them against each other it's on the x-axis we have age and on the y-axis we have percent not alive so as percent not alive goes up that's the indication that the risk of sepsis increases so you can see around midlife there's this clear uptrend of the percent of patients that are not surviving their sepsis episode but before that this risk is relatively low and stable and so just looking at this plot we could probably chop up this data into any number of segments based on this risk so maybe we would do like zero to 40 40 to 60 60 80 100 like whatever but this is just us eyeballing it and it'll be interesting to compare this intuition to what the decision tree is going to spit out okay moving forward we can now train in our decision tree model so here we can Define our number of bins by controlling the maximum number of leaf nodes in our decision tree regressor and the reason this works is that as we saw in the first video a fully grown decision Tree on this data is just massive so you can virtually have any number of bins that you like and it'll work and then finally we just fit our data to the decision tree and now with the decision Tree in hand we can go in and grab all the split values in an automated way so this code is a bit involved so I won't spend too much time on it but for those who are curious you can take a look at it here and it's also available at the GitHub repository linked here but the final result looks something like this so here we have the same plot from before where we have age and years plotted against the percent not alive and so this is qualitatively pretty similar to what we were talking about before like maybe we would have put one here and then adjusted the rest but this is kind of a tricky problem because if I shift this border from here to here to make this first bin look a little better now this bin may not look as good because now you're mixing together these lower risk patients with these higher risk patients and then from a treatment standpoint that may not make a whole lot of sense that's one of the upsides of using a decision tree and leveraging that greedy search to Define these bins because it already is doing that tricky optimization for us and then as a final note I'll just say to take all these with a grain of salt just because the decision tree spits out these optimal age buckets this may not translate well to treatment strategies and so as opposed to just taking this as gospel this is more of a starting place and may just serve better than just arbitrarily drawing lines for these different age groups okay that's it so if you want to learn more be sure to check out the blog published on medium and Linked In the description below feel free to steal the code from the GitHub repository and apply it to use cases or projects that you're working on if you if you enjoyed this content please consider liking subscribing and sharing your thoughts in the comments section below I do read all the comments and I find all the questions and feedback that I receive very valuable and as always thank you for your time and thanks for watching\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from data_ingestion.database import get_video_by_id\n",
    "\n",
    "def format_context(videos: list[dict]) -> str:\n",
    "    \"\"\"Format video data as context string.\"\"\"\n",
    "    parts = []\n",
    "    for video in videos:\n",
    "        part = f\"### {video['title']} (ID: {video['video_id']})\\n\\n{video['transcript']}\"\n",
    "        parts.append(part)\n",
    "    return \"\\n\\n---\\n\\n\".join(parts)\n",
    "\n",
    "def construct_user_message(query: str, video_ids: list[str]) -> str:\n",
    "    \"\"\"Construct user message given a query and video IDs.\"\"\"\n",
    "    # Fetch video data\n",
    "    videos = [get_video_by_id(vid) for vid in video_ids]\n",
    "    videos = [v for v in videos if v]  # filter None\n",
    "    \n",
    "    # Load user template\n",
    "    user_template = Path(\"prompts/answer_user.md\").read_text()\n",
    "    \n",
    "    # Format and return\n",
    "    context = format_context(videos)\n",
    "    return user_template.format(query=query, context=context)\n",
    "\n",
    "# Example usage\n",
    "video_id = \"4vvoIA0MalQ\"\n",
    "query = \"does picking sym4 change how well i find peaks\"\n",
    "user_message = construct_user_message(query, [video_id])\n",
    "print(user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880f719-d6ec-47c8-82c9-c56a5e6223d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
