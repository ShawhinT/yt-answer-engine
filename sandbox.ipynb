{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7dc57d7-07bc-4f45-881f-8bd11bee4f56",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e698586-881b-4e17-8a0c-3f95a7c096ea",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641b0290-a43a-4dd3-a0ff-d637591769f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from data_ingestion.ingest import *\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "from retrieval_eval.query_gen.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d63c5f-02ce-475a-a5b8-8c03c159f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79cd45-d995-4785-9a5e-653ad69b82d5",
   "metadata": {},
   "source": [
    "### ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5422d81c-57a2-4c01-8a53-1bb3fe56d7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4oUOJ37GKYE</td>\n",
       "      <td>The more they hurt you, the stronger you get #...</td>\n",
       "      <td>story is the story of the Hydra monster which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r5qk3uIdkks</td>\n",
       "      <td>What is #ai? — Simply Explained</td>\n",
       "      <td>what is AI exactly I work in the industry and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WgmMK5fS0X0</td>\n",
       "      <td>How to do MORE with LESS - multikills</td>\n",
       "      <td>multitasking isn't a good strategy how do we s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5zeqo-R12vk</td>\n",
       "      <td>How to STAY dumb</td>\n",
       "      <td>in a lot of situations we might feel the need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hB27yAkJLC8</td>\n",
       "      <td>Being fragile means you have more downside tha...</td>\n",
       "      <td>first story is the sword of dimicles is the st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  4oUOJ37GKYE  The more they hurt you, the stronger you get #...   \n",
       "1  r5qk3uIdkks                    What is #ai? — Simply Explained   \n",
       "2  WgmMK5fS0X0              How to do MORE with LESS - multikills   \n",
       "3  5zeqo-R12vk                                   How to STAY dumb   \n",
       "4  hB27yAkJLC8  Being fragile means you have more downside tha...   \n",
       "\n",
       "                                          transcript  \n",
       "0  story is the story of the Hydra monster which ...  \n",
       "1  what is AI exactly I work in the industry and ...  \n",
       "2  multitasking isn't a good strategy how do we s...  \n",
       "3  in a lot of situations we might feel the need ...  \n",
       "4  first story is the sword of dimicles is the st...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('data/videos.db')\n",
    "\n",
    "# View as DataFrame\n",
    "df = pd.read_sql_query(\"SELECT * FROM videos\", conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7371fa3-370b-4388-980d-f115a8dacf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e64ec40-e2fb-4df6-a038-68f75eaab5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = 'vEvytl7wrGM'\n",
    "irow = (df['video_id'] == video_id).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f393b515-e82f-4e06-9531-5970dab65e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey everyone, I'm Shaw. In this video, I'll explain Claude's new skills feature. I'll start by discussing what this is and how it works, highlight how it fits together with existing concepts like MCP and sub agents and then finally walk through a concrete example of using skills with Claude code. So, what are Claude skills? These are just reusable instructions Claude can access when needed. So, we have Claude here. whether you're using it on the web, on your desktop, or through Claude Code. And skills are just bundled special instructions that Claude can automatically access whenever it's relevant to the conversation. You're probably wondering, okay, why does this matter? Why should we care about skills? And it all boils down to a single fact of large language models, which is the clearer your instructions are, the better your results are going to be. What this typically looks like in practice is one of two ways. One is that every time you want to get Claude or any other LLM to do something, you're going to manually write out the instructions or go back and forth with the LLM until it has a clear idea of what you're trying to get it to do. Another slightly better way this might look is that you have these instructions already written out and you have them saved in a folder or a notion template or a Google doc or whatever it is and you simply just grab whatever instruction you need at that moment, copy it and paste it into whatever large language model you're using. While these work well enough most of the time, it can definitely be a lot of work to write instructions from scratch or even go find, copy and paste instructions into your favorite LLM every time you want to have it do something special. And so this is where Claude skills are helpful. They make this process much more streamlined because you can write these instructions once. For example, you have some instructions that tell Claude how you wanted to explain technical concepts to you. Or you have a whole workflow you want Claude to go through to validate SAS ideas. Or you have a set of design principles and tools that you want Claude to use when evaluating your front-end designs. You can just write out these instructions once and then save them as a skill. And then the really cool part is that you don't have to search across a notion document or whatever to go and find these instructions when you want to give them to Claude. Claude is smart enough to look at all the skills it has access to and pick the one that is most relevant to that use case. So we talked about what skills are and why we should care about them. Now let's talk about how they work. The implementation of skills is actually super straightforward. Fundamentally, it's just a folder with a file in it. And so the structure looks like this. You'll have a folder with whatever you want the name of your skill to be. Could be AIT tutor, SAS idea validator, front-end design audit, whatever it is. And then within that folder, you'll have a file called skill.md. The skill file has two main components. The first is its metadata. So the metadata consists of the skills name and a short description for it. And then the second thing is the body. And so this is going to look like a typical prompt in a markdown format, but this will just be those specialized instructions you want Claude to be able to access. And so one cool thing about skills is that since we have these two components, we have the metadata that give us a highle description of what the skill does and then we have the body which are the instructions themselves. This enables a clever way of giving Claude these skills. What this looks like is we'll have Claude here and instead of just dumping the metadata and the body all in the context window at the start of our conversation with Claude, instead what will happen is that Claude will have its system prompt and then the way it gets access to our skills is that just the skill metadata will be added to the context window. This is helpful because the metadata are going to be much shorter than the body or the skills themselves. So the metadata, there's a character limit of I think 64 characters for the name and 1,024 characters for the description. So the metadata is going to be pretty short, while the body can be like 5,000 tokens. And then if the skill is relevant to the conversation or if the user asks Claude to use a specific skill, then it'll read in the body of the skill file. What this unlocks is that now instead of having to be very thoughtful about what skills you give to Claude when because you don't want to overrun its context window with irrelevant things. So, going back to those three examples from before, the AI tutor, the SAS idea validator, and the front-end design audit, these are completely different skills that will be used in completely different contexts. And then, if I'm just dumping all the specialized instructions in the context window at the outset of all my conversations, I'm just going to be wasting a lot of tokens and a lot of Claude's attention on things that aren't relevant to whatever I'm working on in that instance. But with skills, because the metadata are so lightweight, we can have hundreds, if not thousands of skills and have a relatively small impact on the context window. And then if there's a particular skill that's relevant, Claude can call it in as needed. So while this is the simplest skill you can implement, just a single skill.md file with metadata in a body, we can actually do more with this skills feature. We can actually have multiple files in this skill folder. These additional files are more instructions that Claude can read as needed. And so the way that this context is managed is that let's say we have Claude here and it decides that a particular skill is relevant. So it reads the skill body. But then within the skill body, we have a reference to a markdown file called how tooxx.m MD. Claude can go one step further and read the content of how tox feels that it's relevant to whatever it's working on. To make this more concrete, if this is the SAS idea validator and we have specialized instructions for validating BTOC ideas, Claude can call these specialized instructions for validating BTOC ideas. if the user has a business idea that is direct to consumer. But wait, there's more. We're not just limited to adding additional files to this skills folder. We can also add additional folders. And so let's say we have a lot of specialized instructions. So we have how-tos for not just X, but also for Y and Z. We can organize them in a folder. So we can have additional folders where we have some logical organization of information that we want Claude to be able to access whenever it needs it. And so this will work in exactly the same way. In the case where Claude calls one of our skills, it reads the body. And then within the body, we are referencing this howto's folder. And then Claude can read these files one at a time or not at all depending on the context, depending on the conversation. But we're not just limited to giving Claude additional instructions. We can also give Claude specialized tools. And this is because Claude lives in a virtual environment where it has a bash shell with Python installed and NodeJS. So that means Claude can run terminal commands, it can run Python scripts and it can run JavaScript scripts. And so if we have a folder called scripts and in that folder we have a Python file, Claude can call this Python function to do a specific function. And so this will work in the same exact way as giving Claude additional instructions. So we have our skill body that it'll call anytime the skill is relevant. And then within the skill body, we can reference this specialized tool. And then Claude because it has access to the bash shell, it can just run this command python script/fu.py to execute that function. And so skills give us a very simple way to give Claude specialized skills and tools for particular workflows or tasks. And it doesn't do this by just blindly dumping all the skills and tools into the context window at the start of the conversation. It does this one step at a time. And so this is a key feature of skills and is what Anthropic calls progressive disclosure. This is just a fancy way of saying that skills give Claude just enough context for the next step. So instead of dumping everything in the context window up front, content is only injected into the context window when it is needed. And so there actually three levels to this which we've talked about. First level were the metadata living in the skill.md file. This enters the context window right when you boot up claude code or claude on the desktop or wherever you're using claude. And this is going to be limited to about 100 tokens. The next level of this is the body of the skill.md file. And so this enters the context window when Claude invokes that particular skill. This body can be up to 5,000 tokens. But this wasn't the only way we could give Claude specialized instructions. There was also this third level which were all the files and the folders within the skills directory which Claude could access as needed. And then for this content, there's practically no limit to the amount of content you can put in the skills folder. And so ultimately, skills give us better context management without sacrificing capabilities. Before going to the concrete example with claude code, I just wanted to address some confusion people were having between skills and MCP. And so here I want to talk through their differences and when it makes sense to use each. This confusion is warranted because skills and MCP are not mutually exclusive and they actually have a lot of functional overlap and so skills as we've seen provide us a way to give LLM tools and instructions and MCP does the same exact thing. MCP also gives us a way to provide resources and prompts and tools to large language models. But one of the key differences is that skills only work with claude while MCP is a universal open standard. So MCP allows any LLM to talk to any application while skills will only work with Claude. Another key difference is this progressive disclosure. So as we discussed in the previous slide, Claude only gets the context it needs for the next step. While with MCP at the outset, the server will dump all the tools and all the tool metadata into the context window. And so just for context, Notion's MCP server has a wide range of tools and each of those tools has a lot of metadata that are basically self-documenting and self-describing so that the LLM can know when it's appropriate to use a specific tool. All of that is going to be dumped into the context window whether we're using Notion or we're not. And so this is going to be on the order of like 20,000 tokens that's just being dumped into the context window. On the other hand, if we create a skill for notion, at the outset, we're only going to dump the name and the description of our notion skill, which will be on the order of 100 tokens. And so that's two orders of magnitude of context that we're saving because of how skills work. And so despite that, skills and MCP do a lot of the same things. At this point, they have slightly different main use cases. The main use case for skills is teaching Claude how to do things with its available tools. And so Claude comes out of the box with a lot of great tools and maybe you have a few MCP servers that you like that also have a lot of great tools. A great use case for skills is teaching cloud how to better utilize these tools. On the other hand, the main use case for MCP is giving Claude access to complex tools and integrations. And so while we could create a notion skill, this would probably be a lot of work because we'll need to dive into their API, understand how it works, and build custom tooling to do that. And so in these cases where building the tool sets or building out integrations to your favorite software is going to be complicated. Just using an off-the-shelf MCP server is still the way to go. Another thing worth mentioning is how skills and MCP fit together with the sub aents feature in Claude Code. Claude Code is a popular coding agent developed by Anthropic and it has this feature of sub agents which are specialized agents for specific workflows with their own context window. Them having their own context window is really the key feature of sub agents as we'll discuss in a little bit. comparing that to skills is these are specialized instructions and tools for specific workflows while MCP are specialized tool sets for specific workflows. So again, there's a lot of functional overlap between these things. So I wanted to walk through how all of these things will fit together in a typical interaction with cloud code. We'll start with our main cloud code agent. It has default tools. It has its system prompt and it has the UI that we can interact with. And this will all live in a context window. So all the tool calls, the system prompt, our messages, Claude's responses will all live in this context window. And at startup, we'll have our skills. So all the skills that we have preloaded into Claude code. The metadata for the skills will be injected in the context window. And if we have any MCP servers, these will also be injected into the context window. So the main agent basically is a superset of everything in our session. But sometimes we want Claude to do something without bloating our main context window with tokens for a specific task. For example, if we're building some kind of web application and then we want to research a specific library, say fast HTML, we don't necessarily need to do that with the main claude code agent. What we could do instead is to call a sub agent that has its own context window that also sees all the skills that are preloaded into our cloud code, but has a specialized MCP server. Let's say this is the context 7 MCP server that allows cloud to fetch updated documentation. This sub agent can go off research the fast HTML library, understand what libraries are compatible with it and put together a whole specification sheet of what text stack we should use for our web application. And so this will all live in a separate context window and it can just return the result to our main coding agent. And so the main value of sub aents is this better context management. We don't have to run everything in the same context window. We can spin off a new one with the sub aent. The way skills fit into the sub agent is that it will have access to all the skills that we have preloaded into our cloud code account. And then finally, we can give it specific MCP servers so we're not overloading it with all the MCP servers in our cloud code account. So now I want to walk through a concrete example of using cloud skills. Here I'm going to create an AI tutor that will explain technical concepts in plain English. And so the way this will work is we'll have flawed code and then we'll have the specialized skills. So we'll have our skill.md file. We'll have a specialized file that describes how to do research. And then finally, we'll have a custom tool that allows it to pull transcripts from specific YouTube videos. And all the files and code for this are freely available at GitHub at this link here and in the description below. Starting with the skill.md file. So we have our folder called AI tutor and our skill.md file. And our skill MD file looks something like this. So we have the metadata up top. And then we have the instructions. And so this is the name AI tutor. We have a short description. Use when user asks to explain, breakdown or help understand technical concepts, A IML or other technical topics. Makes complex ideas accessible through plain English and narrative structure. And then we have body. This will just be like a typical prompt that will use prompt engineering tricks to write and it'll be in a markdown format. So the next thing we want to look at are these special instructions for doing research. So I have a new file called research methodology.md. And the way Claude knows that this thing exists is that in the skill.md file I have a snippet here. If a concept is unfamiliar or requires research, load research methodology.md for detailed guidance. And so if this situation happens, then Claude can open up research methodology, which is just a regular markdown file. So no metadata here. It'll have its title. Use this guide when you encounter concepts outside your reliable knowledge or when explaining cutting edge developments. When to research, and then I'll have research guidelines within there. Finally, we'll add the special tool that'll allow Claude to get the transcript of specific YouTube videos that it comes across in its research. So here, instead of having reference to this tool in the skill body, it's actually referenced in research methodology.md. So it shows Claude how to actually use this tool. So, it's just by running a command in the terminal and then it will execute this Python script which takes in a video's URL or ID and we'll grab the transcript for it. So, that's everything. Now, let's look at a demo here. I've got cursor opened up and we've got all the contents of the skill folder. So, the skill folder actually lives in your home directory. So, we can actually take a look at it here. So if I do pwd, we'll see for me my claud folder is at my home directory. So it's users shaw.claude skills and then the name of our skill. So this is the AI tutor and then this is the skills file. We have the metadata here. We have the full body. So the instructions on basically how to explain technical ideas in simple terms and just kind of summarizes all my communication tips. So you can see that this is 130 lines or so. We also have the research methodology which is like another 200 lines. So this is why it's helpful to have research methodology separately because if we had it in the main skill file, this would be a very long instructions and not every question is going to require research because it might just be a well-known thing like a neural network or how to do matrix multiplications or how gradient descent works. So, these are well-known technical concepts and they're not things that Claude needs to research about because it's already in its pre-training. It's already something it understands. But for new ideas, it's important that Claude knows how to do research and know when to research. And then finally, we have our scripts folder. This is where we have a Python script that extracts YouTube transcripts. And so, this has some helper functions in it. But ultimately the way it works is that from the command line, Claude can run UV run, run the Python script, pass in the video ID or the video link or whatever it might be. So if you want to read through the code or the instructions, they're all available on GitHub. One final piece, I'm running this locally. And so what I did is for any dependencies I want to install because to get the YouTube transcripts, I'm using this Python library, YouTube transcript API. I'm actually using UV. And so if you're not familiar with UV, I talked about it in a previous video, but it's just a really lightweight and fast Python package and project manager. So I use it for all of my projects. This just ensures that Claude doesn't run into any dependency problems when trying to use this get YouTube transcript tool. So to see this in action, we'll open up the terminal, clear this out, and I'll just type Claude, and I'll just say explain reinforcement learning in simple terms. So let's see if Claude actually uses the skill. Okay, so Claude knew to use the skill on its own. We didn't have to specifically say to. So I'll say yes, proceed. And so it's thinking. And the reason it's thinking is that that's part of the instruction. So, if we go back to the skill.md file, one of the big things about explaining things is that it's important that you explore different possible explanations before giving your explanation. So, I don't want Claude to just jump in and start explaining things. So, that's why I said before responding, think hard, explore multiple narratives, evaluate target audience, choose the best structure, plan your examples, take time to think through these options. So, let's see what it came up with. Reinforcement learning. Learning by trial and error. Okay, I like that. Reinforcement learning is teaching a program to make good decisions by letting it try things, see what happens and learn from the results, how it's different. Okay, so this is good. This is using a narrative structure that I told it which is status quo. So this is how things are. This the baseline. The problem with the baseline traditional programming, you write explicit rules. That's the status quo. What's the problem with that? Stum tasks are impossible to capture in rules. Okay, that's a big problem. Reinforcement learning is a solution. Instead of programming rules, you define a goal and let the program figure out how to achieve it through experience. Perfect. And then it gives a concrete example, which is good. And then it gives why it matters. There's that. But I wanted to do research. So let's say explain GRPO. I don't know if it knows this, but I'm just going to explicitly say do research. Okay. So it read research methodology.md, which is what I wanted. Okay. So it's going to use web search. And so I didn't have to implement a web search tool because that comes out of the box with claude code and claude on the desktop or wherever you're using it. So the way I came up with the skills like all the skill files and the research file and the Python script for the most part I just had Claude do this for me. So I was just using Claude in the browser and I was just talking to it telling it what I wanted it to do. And I basically had it write the skills for me and I went back and forth. Maybe I give it six pieces of feedback until it got to a pretty good spot. Okay, so let's see. GRPO is a smarter way to train language models by having them compete against their own alternative responses rather than relying on absolute quality scores. Status quo. Traditional reinforcement learning from human feedback. Okay, so it's interesting that it chose to use reinforcement learning from human feedback as the starting point. I guess it probably did this because that's probably how Deep Seek framed it in their paper introducing the concept. But this is good. Find a YouTube video by Sha Tbby on this and explain it. It might be called how to train LLMs to think. Let's make sure that this YouTube transcript fetcher works. So, it's going to do a web search. Shot to Lebby. How to train LLMs to think. YouTube gpo. So, now it's going off the rails. Let me see if I can help it out a little bit. So, I guess the idea of the transcript tool is that it can understand transcripts if it's doing a deep dive into a topic. But here, I just want to test if it actually works. Okay, there we go. So, it worked. It ran the script and that is the transcript. And so, now I guess it's going to explain the video. Okay, so that's basically it. Again, all the instructions and the code for this are freely available on GitHub linked in the description. If you have any questions about skills or anything else I talked about in this video, please let me know in the comments section below. And as always, thank you so much for your time and thanks for watching.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['transcript'][irow]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88816fa7-433b-4aaf-a23b-ea3b25b8e8b0",
   "metadata": {},
   "source": [
    "### query gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce0826b-3008-479b-941f-f5e38a35e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ingestion.database import get_video_by_id\n",
    "# from query_gen.functions import get_all_comments, generate_queries\n",
    "\n",
    "# # Get video data\n",
    "# video_data = get_video_by_id(video_id)\n",
    "# print(f\"Title: {video_data['title']}\")\n",
    "\n",
    "# # Get comments\n",
    "# comments = get_all_comments(video_id)\n",
    "# print(f\"Comments: {len(comments)}\")\n",
    "\n",
    "# # Generate queries\n",
    "# result = generate_queries(\n",
    "#   video_title=video_data[\"title\"],\n",
    "#   transcript=video_data[\"transcript\"],\n",
    "#   comments=comments\n",
    "# )\n",
    "\n",
    "# # Display results\n",
    "# for i, q in enumerate(result.queries, 1):\n",
    "#   print(f\"\\n{i}. [{q.query_type.value} / {q.difficulty.value}]\")\n",
    "#   print(f\"   {q.query}\")\n",
    "#   print(f\"   → {q.grounding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32154a97-9de8-4fed-b85a-dee44e751d97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresult\u001b[49m.queries\n",
      "\u001b[31mNameError\u001b[39m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# result.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c14983-6fbe-46dd-aeea-6b2c734fe2fa",
   "metadata": {},
   "source": [
    "### answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18479c-1e51-4dca-9faa-f2df86bd2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.answer import generate_answer\n",
    "\n",
    "# response = generate_answer(\"why is overfitting a problem for decision trees\")\n",
    "# print(response.answer)\n",
    "# for citation in response.citations:\n",
    "#     print(f\"- {citation.title} (https://youtu.be/{citation.video_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8705f9-8263-4e3c-a6ab-1c3356ee7a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
