{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7dc57d7-07bc-4f45-881f-8bd11bee4f56",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e698586-881b-4e17-8a0c-3f95a7c096ea",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641b0290-a43a-4dd3-a0ff-d637591769f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from ingestion.ingest import *\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d63c5f-02ce-475a-a5b8-8c03c159f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761238b-c5e1-457b-a7cb-d766aceb022b",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b5773c-9ae1-4ce1-bccd-0ced278f7c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = \"Gwz4zXPeP_Q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9fd4d7d-d94b-4b46-bedf-a142fa5dbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "params = {\n",
    "    \"part\": \"snippet,contentDetails\",\n",
    "    \"id\": video_id,\n",
    "    \"key\": API_KEY\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "response.raise_for_status()\n",
    "\n",
    "data = response.json()\n",
    "items = data.get(\"items\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3364aba-cfe5-439a-a607-b2421afa683d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'youtube#video',\n",
       " 'etag': 'YVT6WTxx5qXKPJz_7OLIWdAF4xk',\n",
       " 'id': 'Gwz4zXPeP_Q',\n",
       " 'snippet': {'publishedAt': '2020-11-12T22:58:00Z',\n",
       "  'channelId': 'UCa9gErQ9AE5jT2DZLjXBIdA',\n",
       "  'title': 'biometricDahboard3 DEMO',\n",
       "  'description': 'Demo of Fall 2020 UTDesign Project. This is the 3rd iteration of an ongoing project to visualize biometric data in real-time. Application was implemented with Python using Bokeh. The project was sponsored by the Center for Multi-scale Integrated Intelligent Interactive Sensing (MINTS).\\n\\nStudent developers: Vihasreddy Gowreddy, Madhav Mehta, Ryan Rahman, Arjun Sridhar, Rohit Shenoy\\n\\nGitHub: https://github.com/mi3nts/biometricDashboard3\\nProject website: https://mi3nts.github.io/biometricDashboard3/\\n\\nGitHub for 1st Iteration: https://github.com/amnaaaaali/UTDMINTS_BiometricDataApp\\nGitHub for 2nd Iteration: https://github.com/mi3nts/biometricDashboard2\\n\\nUTDesign website: https://utdesign.utdallas.edu/',\n",
       "  'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/Gwz4zXPeP_Q/default.jpg',\n",
       "    'width': 120,\n",
       "    'height': 90},\n",
       "   'medium': {'url': 'https://i.ytimg.com/vi/Gwz4zXPeP_Q/mqdefault.jpg',\n",
       "    'width': 320,\n",
       "    'height': 180},\n",
       "   'high': {'url': 'https://i.ytimg.com/vi/Gwz4zXPeP_Q/hqdefault.jpg',\n",
       "    'width': 480,\n",
       "    'height': 360},\n",
       "   'standard': {'url': 'https://i.ytimg.com/vi/Gwz4zXPeP_Q/sddefault.jpg',\n",
       "    'width': 640,\n",
       "    'height': 480},\n",
       "   'maxres': {'url': 'https://i.ytimg.com/vi/Gwz4zXPeP_Q/maxresdefault.jpg',\n",
       "    'width': 1280,\n",
       "    'height': 720}},\n",
       "  'channelTitle': 'Shaw Talebi',\n",
       "  'categoryId': '22',\n",
       "  'liveBroadcastContent': 'none',\n",
       "  'defaultLanguage': 'en',\n",
       "  'localized': {'title': 'biometricDahboard3 DEMO',\n",
       "   'description': 'Demo of Fall 2020 UTDesign Project. This is the 3rd iteration of an ongoing project to visualize biometric data in real-time. Application was implemented with Python using Bokeh. The project was sponsored by the Center for Multi-scale Integrated Intelligent Interactive Sensing (MINTS).\\n\\nStudent developers: Vihasreddy Gowreddy, Madhav Mehta, Ryan Rahman, Arjun Sridhar, Rohit Shenoy\\n\\nGitHub: https://github.com/mi3nts/biometricDashboard3\\nProject website: https://mi3nts.github.io/biometricDashboard3/\\n\\nGitHub for 1st Iteration: https://github.com/amnaaaaali/UTDMINTS_BiometricDataApp\\nGitHub for 2nd Iteration: https://github.com/mi3nts/biometricDashboard2\\n\\nUTDesign website: https://utdesign.utdallas.edu/'},\n",
       "  'defaultAudioLanguage': 'en'},\n",
       " 'contentDetails': {'duration': 'PT46S',\n",
       "  'dimension': '2d',\n",
       "  'definition': 'hd',\n",
       "  'caption': 'false',\n",
       "  'licensedContent': True,\n",
       "  'contentRating': {},\n",
       "  'projection': 'rectangular'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['items'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bd6e922-6948-4f68-b19c-ec64ab3a07e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = get_transcript('Nm_mmRTpWLg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a02819d5-1094-43de-bdf4-b84ba0365fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey everyone, I'm Shaw. This is the third video in a larger series on AI agents. Here I'll talk about how we can build more powerful automations using LLM workflows. I'll start by reviewing a handful of common design patterns and then walk through a concrete example of implementing an AI virtual assistant using OpenAI's agents SDK. Before talking about LLM workflows, let's first define what a workflow actually is. Here, I'll define it as a set of steps that generate a desired result. So, let's say my desired result is to have an empty inbox. My workflow might look something like this. I'll have an email appear in my inbox. I'll read the email and then if it's something that is important, I'll reply to it. However, if it's not important, I'll delete it. While this is a very simple example, the point is that we all use workflows every single day. And the value of explicitly defining workflows, no matter how simple they may seem, is that this is what allows us to create automations. For example, if I wanted to automate my email workflow, it might look something like this. So instead of just manually reading the emails, I can take the emails, pass them through some kind of email categorizer. This can determine if the emails are junk or not junk. The junks can go to the trash. Then I can have the notjunk emails go through another email categorizer to determine if it's something that I can easily automate with an email writer. But if not, I can just leave it for myself to finish. Ever since we've had computers, there has been a interest and desire to define automations. And a key part of this process is to explicitly define every single step in detail of a workflow. So then you can translate key steps into computer code. However, there is a key limitation in building workflows using just traditional software, which is that building some of these components with rule-based code is hard and limited. For example, to implement a rule-based email classifier, we might look for certain key words that are red flags, or we might analyze the sender's email address. We might see if we've received an email from this person in the past, and so on and so forth. If we were doing this from scratch, it would probably take us some time and some critical thinking. But it's going to be really hard to write a computer program that can be as good as, let's say, a human at doing this classification because there's just a long tale of possible emails that we could receive that's hard to anticipate and account for when writing a computer program like this. However, more recently, getting computers to do things that we want has actually become much easier than before. And this is thanks to large language models. Now, instead of restricting our workflows to only traditional software, we can now build workflows that involve large language models. Looking at that same workflow from before, instead of trying to implement some of these key components with code, we can just plug in a large language model into the workflow. These robots here represent large language models. And so these are things we could get pretty decent performance very quickly by simply writing a well ststructured prompt for each of these tasks. Ever since we've had large language models, people have been trying to figure out ways to incorporate them into software systems like this. And this has kind of given rise to some controversy because some people might see the system and call it an AI agent while some other people might see the system and say this is not an AI agent. However, most people I talked to try to avoid the controversy altogether. So instead of debating whether this is an AI agent or not an AI agent, they adopt this language coined by Andrew Ying of so-called agentic workflows. And this is basically saying that instead of thinking about agents as being this binary thing, thinking of agency on a spectrum. At one end of the spectrum, we have completely rule-based workflows like the one we saw in the previous slide. And then on the other end of the spectrum, we have workflows that have the same level of agency as a human. For example, this system has agency via these LLM components because we're not explicitly instructing the LLM how to classify each and every email or how to respond to each and every email. We just give it guidelines through a prompt. However, this slightly different workflow would have a bit less agency because here we explicitly define the rules for this first categorization and then maybe we have predefined scripts for responding to particular emails. But of course, we could have a little more agency by letting an LLM draft the emails in whatever way it likes. Taking this to the extreme, the most agency would basically be having an LLM as a drop in replacement for the human and it internally and implicitly decides the best way to do this entire workflow in its head. While there are countless ways to create these agentic workflows, there are a handful of common design patterns that have emerged. This is summarized nicely in reference number two, which is a blog post from Anthropic. And so, I actually talked through each of these design patterns in the first video of this series, so I won't spend too much time here. Some basic designs are simply chaining multiple LLMs together or multiple components together or using LLMs to create these bifurcations in the workflow. And so we actually saw examples of both of these in the previous slides. However, we can make things a bit more sophisticated by incorporating parallelization. This can involve taking a task, splitting it into subtasks, and then performing them at the same time to improve latency. or you can perform the same task by multiple LLMs and then have some voting mechanism to have higher quality outputs. However, I want to draw a clear line between these design patterns and these on the right. The key difference is that in these first four patterns, the connections between steps are explicitly defined by the developer. A goes to B, B goes to C. A goes to B which decides whether B can go to C or to D and so on and so forth. The passage of information through the system is predefined. However, with these two design patterns, the flow of information is not necessarily predefined. Rather, it is driven by the outputs of an LLM. For orchestrator workers, you can have an initial LLM take a task, break it down into subtasks, and then decide which steps in the workflow are necessary. This actually comes in two flavors. One is basically the LLM can pass information to other LLMs which are later synthesized. So basically you have the LLM send out information and the information doesn't come back to it. Or you can have the LLM use other LLMs as a tool. Basically, you have this initial model send information to another LLM, which then sends information right back to that initial model. Another way LLMs can drive the flow of information is the so-called evaluator optimizer pattern. Here, an LLM will generate an output. it'll get judged or evaluated by another LLM which will give feedback to the original LLM and then this will basically continuously run until some criteria are satisfied by the output. So the number of times this loop will run is going to be input specific. So there's no way to know at the outset what that number is going to be. So I'll talk more about this evaluator optimizer design in the next video. While we are talking about LLM workflows and LLMs give us this really powerful and flexible way of getting computers to do things that we want, it's important to remember that LLMs aren't a cure all. They have their strengths and they also have their weaknesses. It's important as developers that we understand these strengths and weaknesses and make good design decisions when it comes to building out practical workflows. So a framework that I find helpful when making these decisions is thinking of software as three distinct types. This is based on a famous blog post by Andre Karpathi which is reference number four where he talks about these two kinds of software software 1.0 and software 2.0. More recently people have expanded this discussion to include software 3.0 which is basically large language models. But the story goes like this. Software 1.0 is just code. So the key activity that developers do in order to build systems with code is writing explicit instructions via computer code. So Python, C++, you know, whatever language you like. Software 2.0 also called machine learning is a different way of programming computers. Here instead of explicitly telling the computer what to do with precise instructions, the central activity you do as a developer is you curate high quality training examples. So you can basically teach the computer how to do a particular task. And now today with large language models, we have yet another type of software. Instead of writing code or curating examples, now the central activity is crafting the context to adapt the model's behavior. All three of these things have their strengths and weaknesses. One axis we can look at is predictability. By far traditional code is the best when predictability is important because you know exactly what will happen and why. You can define some input and you can explicitly see the chain of logic that will happen to that input and what is going to get spit out the other side. With machine learning on the other hand you don't really have this predictability because these are stochcastic models. You put in an input and you don't always know what you're going to get on the other side. Of course, machine learning is pretty broad. So, you have very interpretable and predictable models like linear regression and logistic regression. You have things that are a bit more interpretable like a decision trees. And then you have neural networks which are the least interpretable. However, with LLMs, this gets even worse because not only are you using a model to predict some output, like with software 2.0, you know, but with LLMs, you continuously feed the output of the model back into itself to generate text, and that's ultimately the thing that you're going to use. So, there's yet another layer of stochasticity on top of the model prediction itself. The flip side of this is flexibility. Code is the least flexible of the three. The system can only do what you explicitly programmed it to do. With machine learning, this gets a bit more flexible. types of inputs you can pass into it and the types of tasks you can perform with machine learning is much more flexible than what you can typically program a computer to do. And then finally, large language models are the most flexible. They continuously surprise us and they turn out to do things that we didn't realize they can do. So no longer do we have to explicitly tell the computer what to do or explicitly show it examples of what to do. It can do arbitrary tasks by simple prompting. And another important dimension here I would say is computational complexity. So this basically comes down to the cost of running one of these programs. So this is going to be cheapest for code. This is going to involve the least number of operations. This goes up by maybe a couple orders of magnitude with machine learning. And then this goes up even more when working with large language models which are not only massive machine learning models but you run inference for one of these models many different times. So the compute goes up even more when it comes to large language models. And of course there are other axes we can look at but these are the first three that jump to mind for me. You want to understand can I afford a high cost for this component? Is it okay for this component to cost two cents to run or do I really need it to basically be free in order to be practical? Because maybe I need to run it many different times with flexibility. Is it just a handful of different types of inputs that will get fed into this component or is it really unpredictable what types of inputs the component will receive? And then finally with predictability, is it critical that you fully understand and can interpret the behavior of your system in all different situations? or is it okay if there is some ambiguity if that ambiguity allows you to have specific performance? These are all things to consider when trying to decide which type of software should I use for a particular component in my agentic workflow. So I talked a lot about conceptual stuff and abstract stuff. So let's try to make it a bit more concrete with a specific example here. I'm going to build an artificial virtual assistant which I call Ava. As a solo entrepreneur, there are administrative tasks that I need to do, whether that's like sending emails to people or gathering information, creating documents, or whatever it is. Typically, entrepreneurs will hire VAS to help them out, but I am very poor right now. So, wouldn't it be great if I didn't have to pay someone and could just have an AI perform this function for me? This is kind of like the vision. I'm really fascinated by this idea of having email be the interface for an AI application. The idea is I will interact with Ava the same way I might interact with a VA that lives in another state or another country or something like that. So I'll just send her an email and then she'll do things like draft emails for me, update contact information, maybe gather information from a shared knowledge base or something like that. So basically a dropin replacement for a human virtual assistant. The way I'm implementing the initial version of the system is as follows. So I'll take user request which will come from me and then pass this request to a planner agent. The goal of this agent is to break down the task and understand what it is the user's asking for. and it has access to various readonly tools that allow it to gather information and to craft good instructions. A critical output of the planner agent is to decide whether some action is required. For example, if I'm just asking the planner, who are all the AI consultants in my contact list? That doesn't really require an action. It has these readonly tools. So it can gather the information and just send me an email with the information I need. So in that case, the workflow could stop and just send me an email with the information I requested. However, instead, if I asked Ava to write a three-way intro email to these two specific contacts, then it would determine that an action is needed, and it would pass the information along to an executor agent, which has the ability to perform writing actions. So, this could be like drafting an email, creating a new file, or updating someone's contact information. And so this is a very simple workflow and is essentially breaking down the process to perform any administrative task into two key components. A planning component which does like read only actions and a execution component which is actually performing actions in the real world. There are two key benefits in splitting this function into a planning and execution component. First is that the system can do more complex tasks because it isn't going to rely on a single LLM to do it. The second benefit is that the interface between the user and the systems ability to perform actions is buffered by the planner agent. This kind of reduces the risk of the system accidentally sending out random emails or overwriting important information in the knowledge base or something like that. Let's create this agentic workflow. So, I'll be using OpenAI's agents SDK. And then I wrote a handful of custom functions and tools for the agents, which I won't show here because it's just going to be a lot of code to show on the slides, but if you're curious, you can check it out at the GitHub repository. Finally, I'm just importing my OpenAI secret key and some other environment variables through av file, and I'm using thev Python library to do that. Next, we can actually create the agents. As we saw in the previous video of this series, OpenAI defines an agent as an LLM equipped with instructions and tools. These are kind of the two central components here. So, I'll start by writing out the instructions for the planner agent. Read instructions is just a function I wrote to read markdown files from a particular folder. And so, I'm concatenating two markdown files together. One is called ava.md. The other is called planner.md. And this is what the final instructions look like. So I'm just giving some context to the system. You are Ava, a virtual assistant to Sha Lebby, an AI educator and entrepreneur. You help them with administrative tasks. And then I have these additional instructions specifically for the planner. You write instructions for an executor agent to perform user requests. Unlike the executor agent, you only have access to readonly tools. For example, read directory structure and read file contents. Here we're just giving the agent context to its purpose and things that it has access to. One handy functionality of the agents SDK is that we can force agents to have structured outputs. We can do this using pyante. So I'm going to create this custom class which has three attributes to it. First is a boolean flag that says whether execution is required. This is what we're going to use to decide whether to stop the workflow or to pass information on to the executor agent. The next attribute are the instructions for the executor. So if execution is required, this will be a string and these will be the instructions to pass along. And regardless of whether execution is required or not, we'll still want to have a note for the user that'll just be a string. Structured outputs helps give a lot more controllability and predictability to the multi- aent system. We can use this when we create our agent. So this is just this agent class. We'll give it a name. We'll give it those same instructions we saw earlier. And then we can set the output type to that class we just created called planner output. And then here we have two tools that we're giving the agent. One is called red directory structure. Basically, it takes in a folder path and it spits out all the subfolders and the structure of the directory. Another tool is to read the contents of a specific file. If you're curious, this is available at the GitHub. So, you just go to tools/4agents.py and you can see the source code. And then finally, I set the temperature at 0.5. Temperature just controls the randomness of the model's outputs. So, we don't want it to be super random. that is just like gibberish, but we also don't want it to be like super predictable because we want the planner to come up with creative solutions to user requests. Then in a similar way, we can create the executor agent. So we'll define the executor instructions. In a similar way, we'll create the executor. We'll give it its name, give it its instructions, and then we'll give it tools. So ideally the executor won't need to use these read tools because ideally the planner agent gives it all the information it needs to perform the task. But maybe that doesn't happen every single time. So I just included these two tools just in case. The critical tools here involve the ability for the model to write email drafts and save it directly to my Gmail account. Another important tool is giving the model the ability to overwrite existing files. The code is living in this repository which includes various instructions for the agents, different guides and email templates that it can use in crafting drafts and a directory of people that I communicate with frequently which includes things like their name, their email, their URL and a short bio about them. So by giving the executor this tool, it can update the directory, it can update email templates, it can create new email templates. So it just gives the system a lot of capabilities. The key difference with the executor is I set the temperature equal to zero because I don't really need the executor to come up with creative solutions. I just need it to execute exactly what the planning agent tells it to do. So this is not really important to LLM workflows, but it's essential for this specific project, which is setting up the OOTH for Gmail. In order to allow the system to access my inbox and save drafts, there were a few steps I needed to do. So, I need to go to the Google Cloud Console, create a new project, add Gmail's API as a service, and set up my JSON credentials so I can authorize this application to access my Gmail account. And then in Python, I wrote this function called get Gmail service, which would set up the OOTH process. And then with this get Gmail service function, I can just set up the authorization using this script in the main file. So, I'm not going to go in depth on this whole OOTH process because it's not really central to the discussion here, but I do write out all the steps at the GitHub repository in the readme file. So, if you just go to the Google Oath setup, it'll give you step-by-step instructions. It's pretty straightforward once you've done it a couple times, but like I remember the first time setting this up, it was so confusing and I was so lost. If a lot of people are struggling to get this thing set up, happy to do another video on that. But assuming we've got the credentials set up, this is how we can run the workflow. I'll define this asynchronous function called main. The first step will be to grab the user request from this request.txt. So this is just for this initial version. The final vision here is that I'll just send an email to a mailbox called availebi.com or something like that. And that will kick off this whole process for me. But for now, I just put my request in this text file. And then I can run the planner agent using this code here. This is going to give us this result object that we can run in an asynchronous loop, which will basically allow us to stream outputs from the LLM and we'll pass in our request to it. Next, we can process the request. I'll just print some things to the command line to let the user know what's going on. And then a key function here is this handle stream events. This just a custom function I wrote to print various things like when the agent was changed or when a tool was called, what inputs were passed to a tool and to print model outputs. Since it's not like super important to workflows, I won't print it here, but the code is available in the GitHub repository. But basically the planner agent will run and then once the planner agent is done we'll print the response. This final output will have three attributes corresponding to that custom class we created. So we'll print the user note to the user and then we'll see if execution is required. If execution is required this will be true and then we'll run the executor agent. Now we'll pass in the executor agent instructions to the executor agent and then we'll do a similar thing. We'll print executing to the command line and then we'll print the final output of the executor agent. And then again if you're curious about this handle stream events function. You can check out the functions.py file in the GitHub repository. Okay, so let's just see this thing in action. Here's my terminal. I'm going to do Vim so we can see the instructions. So it's kind of like an email. Send call follow-up. Can you send a follow-up email to iffy? And then I just had some meeting notes. Then we can do python ava.py. And then this will run the whole workflow. We can see the planner agent is going. And then we see like it's going to call some tools. So I can pause it real quick. Read directory structure. Read file contents. Read file contents. So it's going to read the directory structure. So this is the directory that it lives in. And then it realized that it needs to use an email template called call follow-up.mmd. And then it also realized it needs to read this directory.csv file to get the contact information for iffy. And then this is what the planner agent said. I will send a follow-up email to iffy using the provided meeting notes. Then it's going to pass it over to the executor agent. Then the executor agent is using this write email draft tool. Here are the arguments. So, it's giving the recipient email, the subject line, and then the body of the email. Finally, the executor agent spits out a message to the user saying that the email draft to Iffy Auna has been created successfully. Seems like everything worked well. So, we'll slide over to my Gmail account, hit refresh, and then we can see indeed the draft was created. Then I can click on that, and I'll zoom in so it's not so small. And then we can actually edit the email before sending it because a lot of times you don't want to just send communications to people straight from LLM. You want to review it and make sure it's something that you want to have your name on. I added my Gmail signature and then I'll add like a couple of small changes cuz these are just raw notes before sending it off. So I can add emojis and you know do whatever I want before sending it. I'll call out a couple of things. So like one is this email guides folder. So, here's where I have some email templates cuz there are a lot of emails that I send that are just like copy paste, change a few things. A lot of those I don't really need to send myself. Ava can send those for me. And then another key thing is this directory.csv. So, this has like contact information and bios for people that I communicate with frequently. One thing I'll end up doing a lot is these like three-way emails between businesses and a handful of AI consultants that I know. This directory is great because I can just tell Ava, this guy's looking for this help. He runs this company. He's looking for people to do this. Can you just send three introductions between him and people in the directory? And it surprisingly did a pretty good job of figuring out how to do that task. And so here we did a pretty simple example, but hopefully it gave you a flavor of what types of things we can do with LLM workflows. The great thing about workflows is that they give us a great balance of capability and predictability and controllability because we can incorporate these guard rails and design the system in such a way so that it does the things that we want it to do in a reliable way. The example here we had this closedended flow. So we had this planning agent which passed information to an executor agent which actually executed the task. Even though there is some magic happening within each of these agents, the overall system is relatively predictable. It's this closedended flow. However, there can be a lot of value in having the so-called open-ended flows. This goes back to the evaluator optimizer paradigm. Instead of just running a step a fixed number of times or in a predetermined way, we can have LLMs receive feedback on their outputs so that it can perform increasingly complex tasks. And so this open-ended flow is going to be the focus of the next video in the series. So, I hope this video was helpful. If you have any questions, please let me know in the comment section below. And as always, thank you so much for your time and thanks for watching.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79cd45-d995-4785-9a5e-653ad69b82d5",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5422d81c-57a2-4c01-8a53-1bb3fe56d7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ytmK_ErTWss</td>\n",
       "      <td>LLMs EXPLAINED in 60 seconds #ai</td>\n",
       "      <td>I'm going to explain large language models in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4oUOJ37GKYE</td>\n",
       "      <td>The more they hurt you, the stronger you get #...</td>\n",
       "      <td>story is the story of the Hydra monster which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O5i_mMUM94c</td>\n",
       "      <td>How I’d learned #datascience (if I had to star...</td>\n",
       "      <td>here's how I'd learn data science if I had to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N3vHJcHBS-w</td>\n",
       "      <td>Model Context Protocol (MCP) Explained in 20 M...</td>\n",
       "      <td>Hey everyone, I'm Shaw. This is the fifth vide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r5qk3uIdkks</td>\n",
       "      <td>What is #ai? — Simply Explained</td>\n",
       "      <td>what is AI exactly I work in the industry and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                              title  \\\n",
       "0  ytmK_ErTWss                   LLMs EXPLAINED in 60 seconds #ai   \n",
       "1  4oUOJ37GKYE  The more they hurt you, the stronger you get #...   \n",
       "2  O5i_mMUM94c  How I’d learned #datascience (if I had to star...   \n",
       "3  N3vHJcHBS-w  Model Context Protocol (MCP) Explained in 20 M...   \n",
       "4  r5qk3uIdkks                    What is #ai? — Simply Explained   \n",
       "\n",
       "                                          transcript  \n",
       "0  I'm going to explain large language models in ...  \n",
       "1  story is the story of the Hydra monster which ...  \n",
       "2  here's how I'd learn data science if I had to ...  \n",
       "3  Hey everyone, I'm Shaw. This is the fifth vide...  \n",
       "4  what is AI exactly I work in the industry and ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('data/videos.db')\n",
    "\n",
    "# View as DataFrame\n",
    "df = pd.read_sql_query(\"SELECT * FROM videos\", conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7371fa3-370b-4388-980d-f115a8dacf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464fd5cb-b3ed-40d8-9611-24f447c116dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transcript\n",
       "22688    1\n",
       "337      1\n",
       "560      1\n",
       "838      1\n",
       "18734    1\n",
       "        ..\n",
       "8881     1\n",
       "22229    1\n",
       "39356    1\n",
       "836      2\n",
       "12202    2\n",
       "Name: count, Length: 155, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['transcript'].str.len().value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f38c9ac0-7a1d-4329-b440-0cceb003986a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    story is the story of the Hydra monster which ...\n",
       "Name: transcript, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['transcript'][df['transcript'].str.len() == df['transcript'].str.len().min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe1e12-e37f-457a-9a4c-a9208b92746b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
